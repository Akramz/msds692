{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to information extraction from text\n",
    "\n",
    "All of the data we've looked at so far has been *structured*, meaning essentially that the data looked like a table or Excel spreadsheet. Not all data looks like that, however.  Human readable text is an extremely common *unstructured* data source.  From the text of a webpage, tweet, or document, businesses want to perform things like:\n",
    "\n",
    "* sentiment analysis\n",
    "* document summarization\n",
    "* document clustering\n",
    "* document recommendation\n",
    "\n",
    "In MSAN 692, data acquisition, we'll learn how to extract the text from webpages or pieces of webpages such as the bestseller list at Amazon. For now, we can play with some prepared text files.\n",
    "\n",
    "Text analysis uses words as data rather than numbers, which means *tokenizing* text; i.e., splitting the text string for a document into individual words. This problem is actually much harder than you might think.  For example, if we split the document text on the space character, then \"San Francisco\" would be split into two words.  For our purposes here, that'll work just fine. See [Tokenization in this excellent information retrieval book](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html) for more information.\n",
    "\n",
    "<img src=\"images/wordcloug.png\" style=\"width:200px\" align=\"right\">The goal of this lecture-lab is to get familiar with tokenizing text and how to extract some basic data, such as word frequency. To do that, we're going to learn about *tuples*, *associations*, and *dictionaries*. To visualize information extracted from a document, we'll use *word clouds* like the image to the right that emphasize words according to their frequency.\n",
    "\n",
    "Let's get started by examining the machinery needed to compute word frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support code\n",
    "\n",
    "Ignore. This is just so that I can use my fun little list of list visualizers in this lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "def listviz(elems, showassoc=True):\n",
    "    s = \"\"\"\n",
    "    digraph G {\n",
    "        nodesep=.05;\n",
    "        node [penwidth=\"0.5\", shape=record,width=.1,height=.1];\n",
    "    \"\"\"\n",
    "    if type(elems)==dict:\n",
    "        elems = elems.items()\n",
    "    labels = []\n",
    "    for i in range(len(elems)):\n",
    "        el = elems[i]\n",
    "        if not el:\n",
    "            labels.append(str(i))\n",
    "        else:\n",
    "            labels.append(idx_elviz(i,el,showassoc))\n",
    "    s += '    mainlist [space=\"0.0\", margin=\"0.01\", fontcolor=\"#444443\", fontname=\"Helvetica\", label=<'+'|'.join(labels)+'>];\\n'\n",
    "\n",
    "    s += \"}\\n\"\n",
    "    return graphviz.Source(s)\n",
    "\n",
    "\n",
    "def lolviz(table, showassoc=True):\n",
    "    \"\"\"\n",
    "    Given a list of lists such as:\n",
    "\n",
    "      [ [('a','3')], [], [('b',230), ('c',21)] ]\n",
    "\n",
    "    return the dot/graphviz to display as a two-dimensional\n",
    "    structure.\n",
    "\n",
    "    If showassoc, display 2-tuples (x,y) as x->y.\n",
    "    \"\"\"\n",
    "    def islol(table):\n",
    "        for x in table:\n",
    "            if type(x)==list or type(x)==tuple:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    if not islol(table):\n",
    "        return listviz(table, showassoc)\n",
    "\n",
    "    s = \"\"\"\n",
    "    digraph G {\n",
    "        nodesep=.05;\n",
    "        rankdir=LR;\n",
    "        node [penwidth=\"0.5\", shape=record,width=.1,height=.1];\n",
    "    \"\"\"\n",
    "    # Make outer list as vertical\n",
    "    labels = []\n",
    "    for i in range(len(table)):\n",
    "        # if (type(bucket)==list or type(bucket)==tuple) and len(bucket) == 0:\n",
    "        #     labels.append(str(i))\n",
    "        # else:\n",
    "        #     labels.append(\"<f%d> %d\" % (i, i))\n",
    "        labels.append(\"<f%d> %d\" % (i, i))\n",
    "\n",
    "    s += '    mainlist [color=\"#444443\", fontsize=\"9\", fontcolor=\"#444443\", fontname=\"Helvetica\", style=filled, fillcolor=\"#D9E6F5\", label = \"'+'|'.join(labels)+'\"];\\n'\n",
    "\n",
    "    # define inner lists\n",
    "    for i in range(len(table)):\n",
    "        bucket = table[i]\n",
    "        if bucket==None:\n",
    "            continue\n",
    "        elements = []\n",
    "        if (type(bucket)==list or type(bucket)==tuple) and len(bucket) == 0:\n",
    "            s += 'node%d [margin=\"0.03\", fontname=\"Italics\", shape=none label=<<font color=\"#444443\" point-size=\"9\">empty list</font>>];\\n' % i\n",
    "        else:\n",
    "            if type(bucket)==list or type(bucket)==tuple:\n",
    "                if len(bucket)>0:\n",
    "                    for j, el in enumerate(bucket):\n",
    "                        elements.append(idx_elviz(j, el, showassoc))\n",
    "            else:\n",
    "                elements.append(elviz(bucket, showassoc))\n",
    "            s += 'node%d [color=\"#444443\", fontname=\"Helvetica\", margin=\"0.01\", space=\"0.0\", shape=record label=<{%s}>];\\n' % (i, '|'.join(elements))\n",
    "\n",
    "    # Do edges\n",
    "    for i in range(len(table)):\n",
    "        bucket = table[i]\n",
    "        if bucket==None:\n",
    "            continue\n",
    "        # if not bucket or ((type(bucket)==list or type(bucket)==tuple) and len(bucket)==0):\n",
    "        #     continue\n",
    "        s += 'mainlist:f%d -> node%d [penwidth=\"0.5\", color=\"#444443\", arrowsize=.4]\\n' % (i,i)\n",
    "    s += \"}\\n\"\n",
    "    # print s\n",
    "    return graphviz.Source(s)\n",
    "\n",
    "\n",
    "def elviz(el, showassoc):\n",
    "    if showassoc and type(el) == tuple and len(el) == 2:\n",
    "        els = \"%s&rarr;%s\" % (elviz(el[0], showassoc), elviz(el[1], showassoc))\n",
    "    elif type(el)==set:\n",
    "        els = '{'+', '.join([str(e) for e in el])+'}'\n",
    "    else:\n",
    "        els = str(el)\n",
    "    els = els.replace('{', '&#123;')\n",
    "    els = els.replace('}', '&#125;')\n",
    "    return els\n",
    "\n",
    "\n",
    "def idx_elviz(idx, el, showassoc):\n",
    "    return \\\n",
    "        \"\"\"\n",
    "        <table BORDER=\"0\" CELLBORDER=\"1\" CELLSPACING=\"0\">\n",
    "          <tr>\n",
    "            <td cellspacing=\"0\" bgcolor=\"#FBFEB0\" border=\"1\" sides=\"b\" valign=\"top\"><font color=\"#444443\" point-size=\"9\">%d</font></td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "            <td bgcolor=\"#FBFEB0\" border=\"0\" align=\"center\"><font point-size=\"11\">%s</font></td>\n",
    "          </tr>\n",
    "        </table>\n",
    "        \"\"\" % (idx, elviz(el,showassoc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Associations and dictionaries\n",
    "\n",
    "In mathematics, we group numbers or other elements in parentheses, thus, forming a *tuple*. For example, to represent a three-dimensional Euclidean point, we'd use 3-tuple notation like `(32,9,9732)`.  Python uses the same mathematical notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'tuple'>\n",
      "(32, 9, 9732)\n"
     ]
    }
   ],
   "source": [
    "p = (32,9,9732)\n",
    "print type(p)\n",
    "print p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Python also uses parentheses for grouping subexpressions like `(1+2)*3`, there is an ambiguity in the language. Does `(5)` represent a single element tuple containing 5 or is it just the integer 5? It turns out that Python considers it an integer so we use the slightly awkward notation `(5,)` instead to mean a 1-tuple.\n",
    "\n",
    "Tuples are ordered and so we access the elements using array indexing notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "9\n",
      "9732\n"
     ]
    }
   ],
   "source": [
    "print p[0]\n",
    "print p[1]\n",
    "print p[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the computational boot camp, we saw one use of tuples in [Manipulating and Visualizing Data](data.ipynb) where the shape of the table is reported using a tuple:\n",
    "\n",
    "```python\n",
    "import pandas\n",
    "prices = pandas.read_csv('data/prices.txt', header=None)\n",
    "print prices.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because tuples are ordered, we could also use list notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = [32,9,9732]\n",
    "print type(q)\n",
    "print q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I first started programming in Python and encountered tuples, they seemed redundant and an unnecessary complexity. But, I've come to appreciate the distinction between tuples and lists. I tend to think of tuples as **unordered**, though, I still have to access the elements using array index notation. In other words, I view tuples as *associations* or grouping of elements. It's often the case that the elements of a single tuple have different types.\n",
    "\n",
    "Relevant to our topic of document analysis, we'll associate a word (string) with the frequency (integer) with which it occurs in the document. For example, if the word \"cat\" appears 10 times, we'd create a tuple like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ('cat', 10)\n",
    "print type(a)\n",
    "print a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuple notation works even when the values are variables not literals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cat', 10)\n",
      "cat\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "word = 'cat'\n",
    "freq = 10\n",
    "a = (word, freq)\n",
    "print a\n",
    "print a[0]\n",
    "print a[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of the elements doesn't really matter but, since we need a way to access the elements, ordering them and using array index notation is the simplest approach. We could define `class`es to access the elements by name, such as `a.word`, but that is beyond what we can cover in this boot camp.\n",
    "\n",
    "**Bag of words representation**\n",
    "\n",
    "A document is a sequence of words that we can represent simply as a list of strings. For example, let's split apart a simple document into words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'sat', 'on', 'the', 'hat']\n"
     ]
    }
   ],
   "source": [
    "doc = 'the cat sat on the hat'\n",
    "words = doc.split(' ')\n",
    "print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a list of words preserves the order, which is sometimes important, but often it is not.  When order is not important, we can use the [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) representation of a document that records the words and their frequencies. This not only compresses the space needed to represent the document but it also tells us something about the meaning of the document.  For example,   examining the most common words is often useful for searching and machine learning problems. One representation for bag of words is just a list of associations (order of tuples doesn't matter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"99pt\" height=\"152pt\"\n",
       " viewBox=\"0.00 0.00 99.01 152.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 148)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-148 95.0054,-148 95.0054,4 -4,4\"/>\n",
       "<!-- mainlist -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>mainlist</title>\n",
       "<polygon fill=\"#d9e6f5\" stroke=\"#444443\" stroke-width=\".5\" points=\"0,-37.5 0,-105.5 21.0054,-105.5 21.0054,-37.5 0,-37.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"10.5027\" y=\"-94.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">0</text>\n",
       "<polyline fill=\"none\" stroke=\"#444443\" stroke-width=\".5\" points=\"0,-88.5 21.0054,-88.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"10.5027\" y=\"-77.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">1</text>\n",
       "<polyline fill=\"none\" stroke=\"#444443\" stroke-width=\".5\" points=\"0,-71.5 21.0054,-71.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"10.5027\" y=\"-60.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">2</text>\n",
       "<polyline fill=\"none\" stroke=\"#444443\" stroke-width=\".5\" points=\"0,-54.5 21.0054,-54.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"10.5027\" y=\"-43.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">3</text>\n",
       "</g>\n",
       "<!-- node0 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>node0</title>\n",
       "<polygon fill=\"none\" stroke=\"#444443\" stroke-width=\".5\" points=\"57.0054,-111.5 57.0054,-143.5 91.0054,-143.5 91.0054,-111.5 57.0054,-111.5\"/>\n",
       "<polygon fill=\"#fbfeb0\" stroke=\"transparent\" points=\"58.5054,-127.5 58.5054,-142.5 77.5054,-142.5 77.5054,-127.5 58.5054,-127.5\"/>\n",
       "<polyline fill=\"none\" stroke=\"#444443\" points=\"58.5054,-127.5 77.5054,-127.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"65.5027\" y=\"-132.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">0</text>\n",
       "<polygon fill=\"#fbfeb0\" stroke=\"transparent\" points=\"58.5054,-112.5 58.5054,-127.5 77.5054,-127.5 77.5054,-112.5 58.5054,-112.5\"/>\n",
       "<text text-anchor=\"start\" x=\"60.3596\" y=\"-116.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"11.00\" fill=\"#000000\">the</text>\n",
       "<polyline fill=\"none\" stroke=\"#444443\" stroke-width=\".5\" points=\"78.0054,-111.5 78.0054,-143.5 \"/>\n",
       "<polygon fill=\"#fbfeb0\" stroke=\"transparent\" points=\"79.5054,-127.5 79.5054,-142.5 90.5054,-142.5 90.5054,-127.5 79.5054,-127.5\"/>\n",
       "<polyline fill=\"none\" stroke=\"#444443\" points=\"79.5054,-127.5 90.5054,-127.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"82.5027\" y=\"-132.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">1</text>\n",
       "<polygon fill=\"#fbfeb0\" stroke=\"transparent\" points=\"79.5054,-112.5 79.5054,-127.5 90.5054,-127.5 90.5054,-112.5 79.5054,-112.5\"/>\n",
       "<text text-anchor=\"start\" x=\"81.9465\" y=\"-116.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"11.00\" fill=\"#000000\">2</text>\n",
       "</g>\n",
       "<!-- mainlist&#45;&gt;node0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>mainlist:f0&#45;&gt;node0</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\".5\" d=\"M21.0054,-97.5C32.6821,-97.5 44.1477,-103.0831 53.4113,-109.4322\"/>\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\".5\" points=\"52.6523,-110.6106 56.7208,-111.7971 54.2803,-108.3324 52.6523,-110.6106\"/>\n",
       "</g>\n",
       "<!-- node1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>node1</title>\n",
       "<polygon fill=\"none\" stroke=\"#444443\" stroke-width=\".5\" points=\"57.5054,-74.5 57.5054,-106.5 90.5054,-106.5 90.5054,-74.5 57.5054,-74.5\"/>\n",
       "<polygon fill=\"#fbfeb0\" stroke=\"transparent\" points=\"58.5054,-90.5 58.5054,-105.5 76.5054,-105.5 76.5054,-90.5 58.5054,-90.5\"/>\n",
       "<polyline fill=\"none\" stroke=\"#444443\" points=\"58.5054,-90.5 76.5054,-90.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"65.0027\" y=\"-95.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">0</text>\n",
       "<polygon fill=\"#fbfeb0\" stroke=\"transparent\" points=\"58.5054,-75.5 58.5054,-90.5 76.5054,-90.5 76.5054,-75.5 58.5054,-75.5\"/>\n",
       "<text text-anchor=\"start\" x=\"60.1685\" y=\"-79.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"11.00\" fill=\"#000000\">cat</text>\n",
       "<polyline fill=\"none\" stroke=\"#444443\" stroke-width=\".5\" points=\"77.5054,-74.5 77.5054,-106.5 \"/>\n",
       "<polygon fill=\"#fbfeb0\" stroke=\"transparent\" points=\"79.0054,-90.5 79.0054,-105.5 90.0054,-105.5 90.0054,-90.5 79.0054,-90.5\"/>\n",
       "<polyline fill=\"none\" stroke=\"#444443\" points=\"79.0054,-90.5 90.0054,-90.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"82.0027\" y=\"-95.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">1</text>\n",
       "<polygon fill=\"#fbfeb0\" stroke=\"transparent\" points=\"79.0054,-75.5 79.0054,-90.5 90.0054,-90.5 90.0054,-75.5 79.0054,-75.5\"/>\n",
       "<text text-anchor=\"start\" x=\"81.4465\" y=\"-79.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"11.00\" fill=\"#000000\">1</text>\n",
       "</g>\n",
       "<!-- mainlist&#45;&gt;node1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>mainlist:f1&#45;&gt;node1</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\".5\" d=\"M21.0054,-80.5C31.8205,-80.5 43.6651,-82.5356 53.4088,-84.7699\"/>\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\".5\" points=\"53.2311,-86.1668 57.4469,-85.7343 53.8816,-83.4434 53.2311,-86.1668\"/>\n",
       "</g>\n",
       "<!-- node2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>node2</title>\n",
       "<polygon fill=\"none\" stroke=\"#444443\" stroke-width=\".5\" points=\"57.5054,-37.5 57.5054,-69.5 90.5054,-69.5 90.5054,-37.5 57.5054,-37.5\"/>\n",
       "<polygon fill=\"#fbfeb0\" stroke=\"transparent\" points=\"58.5054,-53.5 58.5054,-68.5 76.5054,-68.5 76.5054,-53.5 58.5054,-53.5\"/>\n",
       "<polyline fill=\"none\" stroke=\"#444443\" points=\"58.5054,-53.5 76.5054,-53.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"65.0027\" y=\"-58.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">0</text>\n",
       "<polygon fill=\"#fbfeb0\" stroke=\"transparent\" points=\"58.5054,-38.5 58.5054,-53.5 76.5054,-53.5 76.5054,-38.5 58.5054,-38.5\"/>\n",
       "<text text-anchor=\"start\" x=\"60.1685\" y=\"-42.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"11.00\" fill=\"#000000\">sat</text>\n",
       "<polyline fill=\"none\" stroke=\"#444443\" stroke-width=\".5\" points=\"77.5054,-37.5 77.5054,-69.5 \"/>\n",
       "<polygon fill=\"#fbfeb0\" stroke=\"transparent\" points=\"79.0054,-53.5 79.0054,-68.5 90.0054,-68.5 90.0054,-53.5 79.0054,-53.5\"/>\n",
       "<polyline fill=\"none\" stroke=\"#444443\" points=\"79.0054,-53.5 90.0054,-53.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"82.0027\" y=\"-58.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">1</text>\n",
       "<polygon fill=\"#fbfeb0\" stroke=\"transparent\" points=\"79.0054,-38.5 79.0054,-53.5 90.0054,-53.5 90.0054,-38.5 79.0054,-38.5\"/>\n",
       "<text text-anchor=\"start\" x=\"81.4465\" y=\"-42.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"11.00\" fill=\"#000000\">1</text>\n",
       "</g>\n",
       "<!-- mainlist&#45;&gt;node2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>mainlist:f2&#45;&gt;node2</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\".5\" d=\"M21.0054,-62.5C31.7851,-62.5 43.6263,-60.668 53.3768,-58.6571\"/>\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\".5\" points=\"53.8016,-59.9979 57.4185,-57.7891 53.2137,-57.2603 53.8016,-59.9979\"/>\n",
       "</g>\n",
       "<!-- node3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>node3</title>\n",
       "<polygon fill=\"none\" stroke=\"#444443\" stroke-width=\".5\" points=\"57.0054,-.5 57.0054,-32.5 91.0054,-32.5 91.0054,-.5 57.0054,-.5\"/>\n",
       "<polygon fill=\"#fbfeb0\" stroke=\"transparent\" points=\"58.5054,-16.5 58.5054,-31.5 77.5054,-31.5 77.5054,-16.5 58.5054,-16.5\"/>\n",
       "<polyline fill=\"none\" stroke=\"#444443\" points=\"58.5054,-16.5 77.5054,-16.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"65.5027\" y=\"-21.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">0</text>\n",
       "<polygon fill=\"#fbfeb0\" stroke=\"transparent\" points=\"58.5054,-1.5 58.5054,-16.5 77.5054,-16.5 77.5054,-1.5 58.5054,-1.5\"/>\n",
       "<text text-anchor=\"start\" x=\"60.3596\" y=\"-5.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"11.00\" fill=\"#000000\">hat</text>\n",
       "<polyline fill=\"none\" stroke=\"#444443\" stroke-width=\".5\" points=\"78.0054,-.5 78.0054,-32.5 \"/>\n",
       "<polygon fill=\"#fbfeb0\" stroke=\"transparent\" points=\"79.5054,-16.5 79.5054,-31.5 90.5054,-31.5 90.5054,-16.5 79.5054,-16.5\"/>\n",
       "<polyline fill=\"none\" stroke=\"#444443\" points=\"79.5054,-16.5 90.5054,-16.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"82.5027\" y=\"-21.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">1</text>\n",
       "<polygon fill=\"#fbfeb0\" stroke=\"transparent\" points=\"79.5054,-1.5 79.5054,-16.5 90.5054,-16.5 90.5054,-1.5 79.5054,-1.5\"/>\n",
       "<text text-anchor=\"start\" x=\"81.9465\" y=\"-5.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"11.00\" fill=\"#000000\">1</text>\n",
       "</g>\n",
       "<!-- mainlist&#45;&gt;node3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>mainlist:f3&#45;&gt;node3</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\".5\" d=\"M21.0054,-45.5C32.5889,-45.5 44.0417,-40.103 53.3208,-33.9655\"/>\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\".5\" points=\"54.1385,-35.1023 56.6373,-31.6795 52.5493,-32.797 54.1385,-35.1023\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x10e62d1d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag = [('the',2), ('cat',1), ('sat',1), ('hat',1)]\n",
    "lolviz(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That representation is a faithful representation of a bag of words, but looking up word frequencies is not efficient. To find a word, we must linearly scan the list of tuples looking for the word and then plucking out the frequency.\n",
    "\n",
    "**Dictionaries**\n",
    "\n",
    "To look up words quickly, we need to use a dictionary, which is really nothing more than a list of key-value associations. The big difference between a `list` of associations and a `dict` is their internal representation and speed of lookup. To create a dictionary from a list of associations is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(bag)\n",
    "print d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python prints dictionaries out using `dict` literal notation, which we can use to define dictionaries directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dog': 32, 'cat': 99}\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"98pt\" height=\"43pt\"\n",
       " viewBox=\"0.00 0.00 98.00 43.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 39)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-39 94,-39 94,4 -4,4\"/>\n",
       "<!-- mainlist -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>mainlist</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" stroke-width=\".5\" points=\"0,-.5 0,-34.5 90,-34.5 90,-.5 0,-.5\"/>\n",
       "<polygon fill=\"#fbfeb0\" stroke=\"transparent\" points=\"1.5,-18.5 1.5,-33.5 46.5,-33.5 46.5,-18.5 1.5,-18.5\"/>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"1.5,-18.5 46.5,-18.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"21.4973\" y=\"-23.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">0</text>\n",
       "<polygon fill=\"#fbfeb0\" stroke=\"transparent\" points=\"1.5,-1.5 1.5,-18.5 46.5,-18.5 46.5,-1.5 1.5,-1.5\"/>\n",
       "<text text-anchor=\"start\" x=\"3.2058\" y=\"-7.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"11.00\" fill=\"#444443\">dog→32</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" stroke-width=\".5\" points=\"47,-.5 47,-34.5 \"/>\n",
       "<polygon fill=\"#fbfeb0\" stroke=\"transparent\" points=\"48.5,-18.5 48.5,-33.5 89.5,-33.5 89.5,-18.5 48.5,-18.5\"/>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"48.5,-18.5 89.5,-18.5 \"/>\n",
       "<text text-anchor=\"start\" x=\"66.4973\" y=\"-23.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">1</text>\n",
       "<polygon fill=\"#fbfeb0\" stroke=\"transparent\" points=\"48.5,-1.5 48.5,-18.5 89.5,-18.5 89.5,-1.5 48.5,-1.5\"/>\n",
       "<text text-anchor=\"start\" x=\"50.0454\" y=\"-7.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"11.00\" fill=\"#444443\">cat→99</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x10e66dcd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = {'dog':32, 'cat':99}\n",
    "print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pythontutor.com visualizes the data structures in different ways that highlights dictionaries as lookup tables.\n",
    "\n",
    "<img src=\"figures/list-assoc.png\" style=\"width:300px\"><img src=\"figures/dict-assoc.png\" style=\"width:240px\">\n",
    "\n",
    "In implementation, however, dictionaries are actually more complicated than lists of associations in order to get the speed. More on this in MSAN692 Data Acquisition. \n",
    "\n",
    "Accessing elements of the dictionary looks like array indexing except that the index is an arbitrary object, such as a string in our case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print d['the']\n",
    "print d['hat']\n",
    "d['hat'] = 99    # alter the association\n",
    "print d['hat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to access a key that does not exist in the dictionary causes a `KeyError` so it's best to check if the key exists first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print d['foo']     This would cause a KeyError!\n",
    "if 'cat' in d:       # hat is indeed in dictionary d\n",
    "    print d['cat']\n",
    "if 'foo' in d:       # does not exist so we don't get an error on the next line    \n",
    "    print d['foo']   # does not execute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ever need a list of association representation of a dictionary, use function `items()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print d.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know about associating words and word frequencies using dictionaries, we can try pulling apart a document into words.\n",
    "\n",
    "## Tokenizing a document\n",
    "\n",
    "Let's use an article on Istanbul as our text file and then figure out how to get an appropriate list of words out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head data/IntroIstanbul.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Loading files](files.md), we learned how to read the contents of such a file into a string and split it on the space character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = open('data/IntroIstanbul.txt')\n",
    "contents = f.read() # read all content of the file\n",
    "f.close()\n",
    "words = contents.split(' ')\n",
    "print words[:25]    # print first 25 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an exercise, we filtered out the empty strings `''` and newline strings `'\\n'` using the filter pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in words if len(w)>1]\n",
    "print words[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks more like it although it is still not very clean. Some of the words have `\\n` or punctuation on the end and some words are capitalized.  What we need, though, is all words normalized so that `people` and `People` are consider the same word etc...\n",
    "\n",
    "**Exercise**:  Implement another filter pattern to convert the words to lowercase using `lower()`. E.g., `'The'.lower()` is `'the'`.\n",
    "\n",
    "Here's one way to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w.lower() for w in words]\n",
    "print words[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:  Implement another filter pattern to get rid of the '\\n' using `split()`. E.g., `'part\\n'.strip()` gives 'part'.\n",
    "\n",
    "Here's one way to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w.strip() for w in words]\n",
    "print words[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not the best we can do, but it's good enough for the moment. \n",
    "\n",
    "## Computing word frequencies\n",
    "\n",
    "Let's create a bag of words representation. My work plan would have a description like \"Walk through the words in a document, updating a dictionary that holds the count for each word.\" The plan pseudocode would have a loop over the words whose body incremented a count in a dictionary\n",
    "\n",
    "1. let wfreqs be an empty dictionary mapping words to word counts\n",
    "2. for each word w in words:<br>if w not in wfreqs, let wfreqs[w] = 0.<br>Otherwise add one to wfreqs[w].\n",
    "\n",
    "My code implementation would look like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfreqs = {}\n",
    "for w in words:\n",
    "    if w not in wfreqs: wfreqs[w] = 1\n",
    "    else: wfreqs[w] = wfreqs[w] + 1\n",
    "print wfreqs['ottoman']\n",
    "print wfreqs['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the frequency of elements in a list is common enough that Python provides a built-in data structure called a `Counter` that will do this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "wfreqs = Counter(words)\n",
    "print wfreqs['ottoman']\n",
    "print wfreqs['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That data structure is nice because it can give the list of, say, 10 most common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print wfreqs.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word clouds\n",
    "\n",
    "Python has a nice library called `wordcloud` we can use to visualize the relative frequency of words. It should already be installed in your Anaconda Python directory, but if notm use the command line to install it:\n",
    "\n",
    "```bash\n",
    "$ pip install wordcloud\n",
    "```\n",
    "\n",
    "The key elements of the following code are the creation of the `WordCloud` and calling `fit_words()` with a dictionary (type `dict`) of word-freq associations, `wfreq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud()\n",
    "wordcloud.fit_words(wfreqs)\n",
    "\n",
    "fig=plt.figure(figsize=(5, 3))   # Prepare a plot 5x3 inches\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's kind of busy will all of those words and there, so let's focus on the top 30 words. To do that we will call `most_common()`, which gives us a list of tuples. Because `fit_words()` it requires a `dict`, we converted the most common word list into a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 30 most common word-freq pairs then convert to dictionary for use by WordCloud\n",
    "wtuples = wfreqs.most_common(30)\n",
    "wdict = dict(wtuples)\n",
    "\n",
    "wordcloud = WordCloud()\n",
    "wordcloud.fit_words(wdict)\n",
    "\n",
    "fig=plt.figure(figsize=(5, 3))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better but it looks like common English words like \"the\" and \"of\" are dominating the visualization. To focus on the words most relevant to the document, let's filter out such so-called English *stop words*. [scikit-learn](http://scikit-learn.org/stable/), a machine learning library you will become very familiar with in future classes, provides a nice list of stop words we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "english = list(ENGLISH_STOP_WORDS) # Convert to a list so I can grab a subset\n",
    "print english[:25]                 # Print 25 of the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "Filter out the English stop words from the `words` list we computed above and reset `wfreqs` to a `Counter` based off this filtered list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = [w for w in words if w not in ENGLISH_STOP_WORDS]\n",
    "wfreqs = Counter(words)\n",
    "print wfreqs.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Display a word cloud for `wfreqs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtuples = wfreqs.most_common(30)\n",
    "wdict = dict(wtuples)\n",
    "\n",
    "wordcloud = WordCloud()\n",
    "wordcloud.fit_words(wdict)\n",
    "\n",
    "fig=plt.figure(figsize=(5, 3))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play around with the list of stop words to remove things like \"largest\" and others to really get the key words to pop out. There is a technique to automatically damp down common English words called [TFIDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), which we will learn about in MSAN692 Data Acquisition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary\n",
    "\n",
    "Text files are an unstructured data source that we typically represent as a bag of words. A bag of words representation is a set of associations mapping words to their frequency or count. We typically use a dictionary data structure for bag of words because dictionary lookup is extremely efficient, versus linearly scanning an entire list of associations. We used word clouds to visualize the relative frequency of words in a document. \n",
    "\n",
    "The data structures and techniques described in this lecture-lab form the basis of natural language processing (NLP)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "189px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
